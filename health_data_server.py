"""
ğŸŒ Complete Cloud Health Management Server v3.1 - CSV Analysis Enhanced
ãƒ­ãƒ¼ã‚«ãƒ«ä¾å­˜ã‚¼ãƒ­ - å®Œå…¨ã‚¯ãƒ©ã‚¦ãƒ‰åŒ–çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

ã€ä¿®æ­£å†…å®¹ã€‘:
- ãƒ­ã‚°å‡ºåŠ›å¼·åŒ–ï¼ˆgunicornå¯¾å¿œï¼‰
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©³ç´°åŒ–
- ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¿½åŠ 
- å‡¦ç†çŠ¶æ³å¯è¦–åŒ–
- CSVå†…å®¹è¡¨ç¤ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ 

æ©Ÿèƒ½:
- HAEãƒ‡ãƒ¼ã‚¿å—ä¿¡ â†’ å³åº§å¤‰æ›ãƒ»çµ±åˆ â†’ åˆ†æ â†’ LINEé€šçŸ¥
- CSVå†…å®¹è¡¨ç¤ºãƒ»æœŸé–“åˆ¥ãƒ‡ãƒ¼ã‚¿ç¢ºèª
- å®šæœŸç›£è¦–ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
- å…¨å‡¦ç†ã‚’Railwayç’°å¢ƒã§å®Œçµ

Railwayç’°å¢ƒå¤‰æ•°ï¼ˆå¿…é ˆï¼‰:
- LINE_BOT_CHANNEL_ACCESS_TOKEN
- LINE_USER_ID  
- OURA_ACCESS_TOKEN (Optional)
"""

from flask import Flask, request, jsonify
import json
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
import os
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
import threading
import time
import traceback
import logging
import sys

# ===== ãƒ­ã‚°è¨­å®šå¼·åŒ– =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.StreamHandler(sys.stderr)
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ===== ç’°å¢ƒè¨­å®š =====
DATA_DIR = os.path.join(os.path.dirname(__file__), 'health_api_data')
REPORTS_DIR = os.path.join(os.path.dirname(__file__), 'reports')

# Railway Volumeå¯¾å¿œ
if os.path.exists('/app/reports'):
    REPORTS_DIR = '/app/reports'
if os.path.exists('/app/health_api_data'):
    DATA_DIR = '/app/health_api_data'

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORTS_DIR, exist_ok=True)

# ç’°å¢ƒå¤‰æ•°
LINE_BOT_TOKEN = os.environ.get('LINE_BOT_CHANNEL_ACCESS_TOKEN')
LINE_USER_ID = os.environ.get('LINE_USER_ID')
OURA_TOKEN = os.environ.get('OURA_ACCESS_TOKEN')

logger.info(f"ğŸš€ çµ±åˆã‚µãƒ¼ãƒãƒ¼v3.1åˆæœŸåŒ–é–‹å§‹")
logger.info(f"ğŸ“ DATA_DIR: {DATA_DIR}")
logger.info(f"ğŸ“ REPORTS_DIR: {REPORTS_DIR}")
logger.info(f"ğŸ” LINEè¨­å®š: {'âœ…å®Œäº†' if LINE_BOT_TOKEN and LINE_USER_ID else 'âŒè¦è¨­å®š'}")
logger.info(f"ğŸ” Ouraè¨­å®š: {'âœ…å®Œäº†' if OURA_TOKEN else 'âŒæœªè¨­å®š'}")

# ===== HAEãƒ‡ãƒ¼ã‚¿å¤‰æ›æ©Ÿèƒ½ =====
class HAEDataConverter:
    """HAEãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã«å¤‰æ›"""
    
    METRIC_MAPPING = {
        'weight_body_mass': 'ä½“é‡_kg',
        'lean_body_mass': 'ç­‹è‚‰é‡_kg', 
        'body_fat_percentage': 'ä½“è„‚è‚ªç‡',
        'dietary_energy': 'æ‘‚å–ã‚«ãƒ­ãƒªãƒ¼_kcal',
        'basal_energy_burned': 'åŸºç¤ä»£è¬_kcal',
        'active_energy': 'æ´»å‹•ã‚«ãƒ­ãƒªãƒ¼_kcal',
        'step_count': 'æ­©æ•°',
        'sleep_analysis': 'ç¡çœ æ™‚é–“_hours',
        'protein': 'ã‚¿ãƒ³ãƒ‘ã‚¯è³ª_g',
        'carbohydrates': 'ç³–è³ª_g',
        'fiber': 'é£Ÿç‰©ç¹Šç¶­_g',
        'total_fat': 'è„‚è³ª_g'
    }
    
    def convert_hae_to_daily_row(self, hae_data: Dict) -> Dict:
        """HAE JSONã‚’æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿è¡Œã«å¤‰æ›"""
        try:
            logger.info("ğŸ”„ HAEãƒ‡ãƒ¼ã‚¿å¤‰æ›é–‹å§‹")
            metrics = hae_data.get('data', {}).get('metrics', [])
            logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {len(metrics)}")
            
            # åŸºæœ¬è¡Œãƒ‡ãƒ¼ã‚¿
            daily_row = {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'ä½“é‡_kg': None,
                'ç­‹è‚‰é‡_kg': None,
                'ä½“è„‚è‚ªé‡_kg': None,
                'ä½“è„‚è‚ªç‡': None,
                'ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal': None,
                'æ‘‚å–ã‚«ãƒ­ãƒªãƒ¼_kcal': None,
                'æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼_kcal': None,
                'åŸºç¤ä»£è¬_kcal': None,
                'æ´»å‹•ã‚«ãƒ­ãƒªãƒ¼_kcal': None,
                'æ­©æ•°': None,
                'ç¡çœ æ™‚é–“_hours': None,
                'ä½“è¡¨æ¸©åº¦_celsius': None,
                'ä½“è¡¨æ¸©å¤‰åŒ–_celsius': None,
                'ä½“è¡¨æ¸©åå·®_celsius': None,
                'ä½“è¡¨æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_celsius': None,
                'ã‚¿ãƒ³ãƒ‘ã‚¯è³ª_g': None,
                'ç³–è³ª_g': None,
                'é£Ÿç‰©ç¹Šç¶­_g': None,
                'è„‚è³ª_g': None,
                'oura_total_calories': None,
                'oura_estimated_basal': None,
                'total_calories_updated': None,
                'calculation_method': 'HAE_AUTO'
            }
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¤‰æ›
            converted_count = 0
            for metric in metrics:
                name = metric.get('name', '')
                if name in self.METRIC_MAPPING:
                    csv_column = self.METRIC_MAPPING[name]
                    data_points = metric.get('data', [])
                    
                    if data_points:
                        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                        latest_point = data_points[-1]
                        daily_row[csv_column] = latest_point.get('qty')
                        converted_count += 1
                        logger.info(f"âœ… {name} â†’ {csv_column}: {latest_point.get('qty')}")
            
            logger.info(f"ğŸ¯ å¤‰æ›å®Œäº†: {converted_count}å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
            
            # ä½“è„‚è‚ªé‡è¨ˆç®—
            if daily_row['ä½“é‡_kg'] and daily_row['ä½“è„‚è‚ªç‡']:
                daily_row['ä½“è„‚è‚ªé‡_kg'] = daily_row['ä½“é‡_kg'] * (daily_row['ä½“è„‚è‚ªç‡'] / 100)
                logger.info(f"ğŸ’¡ ä½“è„‚è‚ªé‡è¨ˆç®—: {daily_row['ä½“è„‚è‚ªé‡_kg']:.2f}kg")
            
            # ã‚«ãƒ­ãƒªãƒ¼åæ”¯è¨ˆç®—
            intake = daily_row['æ‘‚å–ã‚«ãƒ­ãƒªãƒ¼_kcal'] or 0
            basal = daily_row['åŸºç¤ä»£è¬_kcal'] or 0
            active = daily_row['æ´»å‹•ã‚«ãƒ­ãƒªãƒ¼_kcal'] or 0
            
            if intake and (basal or active):
                daily_row['æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼_kcal'] = basal + active
                daily_row['ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal'] = intake - (basal + active)
                logger.info(f"ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼åæ”¯: {daily_row['ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal']}kcal")
            
            logger.info("âœ… HAEãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Œäº†")
            return daily_row
            
        except Exception as e:
            logger.error(f"âŒ HAEãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return None

# ===== CSVçµ±åˆæ©Ÿèƒ½ =====
class CSVDataIntegrator:
    """HAEãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜CSVã«çµ±åˆ"""
    
    def __init__(self):
        self.reports_dir = Path(REPORTS_DIR)
        self.daily_csv = self.reports_dir / "æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿.csv"
        self.ma7_csv = self.reports_dir / "7æ—¥ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿.csv"
        self.index_csv = self.reports_dir / "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿.csv"
        logger.info(f"ğŸ“Š CSVçµ±åˆæ©Ÿèƒ½åˆæœŸåŒ–: {self.reports_dir}")
    
    def integrate_daily_data(self, daily_row: Dict) -> bool:
        """æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«çµ±åˆ"""
        try:
            logger.info("ğŸ”„ CSVçµ±åˆé–‹å§‹")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            if self.daily_csv.exists():
                df = pd.read_csv(self.daily_csv, encoding='utf-8-sig')
                logger.info(f"ğŸ“– æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}è¡Œ")
            else:
                df = pd.DataFrame()
                logger.info("ğŸ“ æ–°è¦ãƒ‡ãƒ¼ã‚¿ä½œæˆ")
            
            # æ–°ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆåŒä¸€æ—¥ä»˜ã¯ä¸Šæ›¸ãï¼‰
            new_df = pd.DataFrame([daily_row])
            if not df.empty:
                df = df[df['date'] != daily_row['date']]  # æ—¢å­˜ã®åŒæ—¥ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
                df = pd.concat([df, new_df], ignore_index=True)
            else:
                df = new_df
            
            # æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆ
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            df['date'] = df['date'].dt.strftime('%Y-%m-%d')
            
            # ä¿å­˜
            df.to_csv(self.daily_csv, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ’¾ æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {len(df)}è¡Œ")
            
            # ç§»å‹•å¹³å‡å†è¨ˆç®—
            self.recalculate_moving_averages(df)
            
            logger.info("âœ… CSVçµ±åˆå®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ CSVçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def recalculate_moving_averages(self, df: pd.DataFrame):
        """ç§»å‹•å¹³å‡å†è¨ˆç®—"""
        try:
            logger.info("ğŸ”„ ç§»å‹•å¹³å‡è¨ˆç®—é–‹å§‹")
            
            # æ•°å€¤ã‚«ãƒ©ãƒ ã®ç§»å‹•å¹³å‡è¨ˆç®—
            numeric_columns = ['ä½“é‡_kg', 'ç­‹è‚‰é‡_kg', 'ä½“è„‚è‚ªé‡_kg', 'ä½“è„‚è‚ªç‡', 
                             'ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal', 'æ‘‚å–ã‚«ãƒ­ãƒªãƒ¼_kcal', 'æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼_kcal',
                             'åŸºç¤ä»£è¬_kcal', 'æ´»å‹•ã‚«ãƒ­ãƒªãƒ¼_kcal', 'æ­©æ•°', 'ç¡çœ æ™‚é–“_hours']
            
            calculated_count = 0
            for col in numeric_columns:
                if col in df.columns:
                    df[f'{col}_ma7'] = df[col].rolling(window=7, min_periods=1).mean()
                    df[f'{col}_ma14'] = df[col].rolling(window=14, min_periods=1).mean()
                    df[f'{col}_ma28'] = df[col].rolling(window=28, min_periods=1).mean()
                    calculated_count += 1
            
            # ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            df.to_csv(self.ma7_csv, index=False, encoding='utf-8-sig')
            df.to_csv(self.index_csv, index=False, encoding='utf-8-sig')
            
            logger.info(f"âœ… ç§»å‹•å¹³å‡è¨ˆç®—å®Œäº†: {calculated_count}ã‚«ãƒ©ãƒ å‡¦ç†")
            
        except Exception as e:
            logger.error(f"âŒ ç§»å‹•å¹³å‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())

# ===== å¥åº·åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ =====
class HealthAnalyticsEngine:
    """å¥åº·æŒ‡æ¨™åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.reports_dir = Path(REPORTS_DIR)
        self.target_body_fat_rate = 12.0
        logger.info(f"ğŸ§  å¥åº·åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆç›®æ¨™ä½“è„‚è‚ªç‡: {self.target_body_fat_rate}%ï¼‰")
    
    def analyze_health_data(self) -> Optional[Dict]:
        """å¥åº·ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œ"""
        try:
            logger.info("ğŸ”„ å¥åº·åˆ†æé–‹å§‹")
            
            ma7_file = self.reports_dir / "7æ—¥ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿.csv"
            if not ma7_file.exists():
                logger.error("âŒ ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            df = pd.read_csv(ma7_file, encoding='utf-8-sig')
            if df.empty:
                logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return None
            
            logger.info(f"ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ")
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            latest = df.iloc[-1]
            logger.info(f"ğŸ“… æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {latest.get('date')}")
            
            # åˆ†æçµæœä½œæˆ
            report = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'current_body_fat_rate': latest.get('ä½“è„‚è‚ªç‡'),
                'target_body_fat_rate': self.target_body_fat_rate,
                'body_fat_progress': {
                    '7day_change': latest.get('ä½“è„‚è‚ªç‡') - df.iloc[-8].get('ä½“è„‚è‚ªç‡', latest.get('ä½“è„‚è‚ªç‡')) if len(df) >= 8 else 0,
                    '14day_change': latest.get('ä½“è„‚è‚ªç‡') - df.iloc[-15].get('ä½“è„‚è‚ªç‡', latest.get('ä½“è„‚è‚ªç‡')) if len(df) >= 15 else 0,
                    '28day_change': latest.get('ä½“è„‚è‚ªç‡') - df.iloc[-29].get('ä½“è„‚è‚ªç‡', latest.get('ä½“è„‚è‚ªç‡')) if len(df) >= 29 else 0
                },
                'body_composition': {
                    'weight': latest.get('ä½“é‡_kg'),
                    'muscle_mass': latest.get('ç­‹è‚‰é‡_kg'),
                    'body_fat_mass': latest.get('ä½“è„‚è‚ªé‡_kg')
                },
                'calorie_balance': {
                    'current': latest.get('ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal'),
                    '7day_avg': latest.get('ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal_ma7'),
                    '14day_avg': latest.get('ã‚«ãƒ­ãƒªãƒ¼åæ”¯_kcal_ma14')
                }
            }
            
            # åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = self.reports_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file.name}")
            logger.info(f"ğŸ¯ ç¾åœ¨ä½“è„‚è‚ªç‡: {report['current_body_fat_rate']}%")
            logger.info("âœ… å¥åº·åˆ†æå®Œäº†")
            return report
            
        except Exception as e:
            logger.error(f"âŒ å¥åº·åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return None

# ===== LINEé€šçŸ¥æ©Ÿèƒ½ =====
class LineBotNotifier:
    """LINE Bot APIé€šçŸ¥"""
    
    def __init__(self):
        self.token = LINE_BOT_TOKEN
        self.user_id = LINE_USER_ID
        self.api_url = "https://api.line.me/v2/bot/message/push"
        logger.info(f"ğŸ“± LINEé€šçŸ¥æ©Ÿèƒ½åˆæœŸåŒ–ï¼ˆè¨­å®š: {'âœ…å®Œäº†' if self.token and self.user_id else 'âŒä¸å®Œå…¨'}ï¼‰")
    
    def send_health_report(self, report: Dict) -> bool:
        """å¥åº·ãƒ¬ãƒãƒ¼ãƒˆã‚’LINEé€ä¿¡"""
        if not self.token or not self.user_id:
            logger.error("âŒ LINEè¨­å®šãŒä¸å®Œå…¨ã§ã™")
            return False
        
        try:
            logger.info("ğŸ”„ LINEé€šçŸ¥é€ä¿¡é–‹å§‹")
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = self.format_health_message(report)
            logger.info(f"ğŸ“ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆå®Œäº†ï¼ˆ{len(message)}æ–‡å­—ï¼‰")
            
            headers = {
                'Authorization': f'Bearer {self.token}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'to': self.user_id,
                'messages': [{'type': 'text', 'text': message}]
            }
            
            logger.info("ğŸš€ LINE APIå‘¼ã³å‡ºã—å®Ÿè¡Œ")
            response = requests.post(self.api_url, headers=headers, 
                                   data=json.dumps(data), timeout=10)
            
            if response.status_code == 200:
                logger.info("âœ… LINEé€šçŸ¥é€ä¿¡å®Œäº†")
                return True
            else:
                logger.error(f"âŒ LINEé€šçŸ¥å¤±æ•—: {response.status_code}")
                logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def format_health_message(self, report: Dict) -> str:
        """å¥åº·ãƒ¬ãƒãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ"""
        current_bf = report.get('current_body_fat_rate', 0)
        target_bf = report.get('target_body_fat_rate', 12.0)
        progress = report.get('body_fat_progress', {})
        
        message = f"""ğŸ“Šä½“è„‚è‚ªç‡é€²æ— | {report.get('timestamp', 'N/A')}

ğŸ¯ {current_bf:.1f}%

ç¾åœ¨: {current_bf:.1f}%  ç›®æ¨™: {target_bf:.1f}%
28æ—¥: {progress.get('28day_change', 0):+.1f}%  14æ—¥: {progress.get('14day_change', 0):+.1f}%  7æ—¥: {progress.get('7day_change', 0):+.1f}%

ğŸ’ªä½“çµ„æˆå¤‰åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰
ä½“é‡: {report.get('body_composition', {}).get('weight', 0):.1f}kg
ç­‹è‚‰é‡: {report.get('body_composition', {}).get('muscle_mass', 0):.1f}kg
ä½“è„‚è‚ªé‡: {report.get('body_composition', {}).get('body_fat_mass', 0):.1f}kg

ğŸ”¥ã‚«ãƒ­ãƒªãƒ¼åæ”¯çŠ¶æ³
ç¾åœ¨: {report.get('calorie_balance', {}).get('current', 0):.0f}kcal
7æ—¥å¹³å‡: {report.get('calorie_balance', {}).get('7day_avg', 0):.0f}kcal

ã€v3.1ã€‘çµ±åˆã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­âœ…"""

        return message

# ===== çµ±åˆå‡¦ç†ã‚¯ãƒ©ã‚¹ =====
class CompleteProcessor:
    """çµ±åˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.converter = HAEDataConverter()
        self.integrator = CSVDataIntegrator()
        self.analytics = HealthAnalyticsEngine()
        self.notifier = LineBotNotifier()
        logger.info("ğŸš€ çµ±åˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def process_hae_data_complete(self, hae_data: Dict) -> bool:
        """HAEãƒ‡ãƒ¼ã‚¿å—ä¿¡ã‹ã‚‰é€šçŸ¥ã¾ã§å®Œå…¨å‡¦ç†"""
        try:
            logger.info("ğŸ¯ ===== çµ±åˆå‡¦ç†é–‹å§‹ =====")
            
            # 1. HAE â†’ CSVå¤‰æ›
            logger.info("ã€STEP 1ã€‘ HAEãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Ÿè¡Œ")
            daily_row = self.converter.convert_hae_to_daily_row(hae_data)
            if not daily_row:
                logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å¤‰æ›å¤±æ•—")
                return False
            
            # 2. CSVçµ±åˆãƒ»ç§»å‹•å¹³å‡
            logger.info("ã€STEP 2ã€‘ CSVçµ±åˆãƒ»ç§»å‹•å¹³å‡å®Ÿè¡Œ")
            if not self.integrator.integrate_daily_data(daily_row):
                logger.error("âŒ CSVçµ±åˆå¤±æ•—")
                return False
            
            # 3. å¥åº·åˆ†æ
            logger.info("ã€STEP 3ã€‘ å¥åº·åˆ†æå®Ÿè¡Œ")
            report = self.analytics.analyze_health_data()
            if not report:
                logger.error("âŒ å¥åº·åˆ†æå¤±æ•—")
                return False
            
            # 4. LINEé€šçŸ¥
            logger.info("ã€STEP 4ã€‘ LINEé€šçŸ¥å®Ÿè¡Œ")
            if not self.notifier.send_health_report(report):
                logger.warning("âš ï¸ LINEé€šçŸ¥å¤±æ•—ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰")
                # é€šçŸ¥å¤±æ•—ã§ã‚‚å‡¦ç†ã¯æˆåŠŸã¨ã™ã‚‹
            
            logger.info("ğŸ‰ ===== çµ±åˆå‡¦ç†å®Œäº† =====")
            return True
            
        except Exception as e:
            logger.error(f"âŒ çµ±åˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return False

# ===== ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±åˆå‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ =====
processor = CompleteProcessor()

# ===== Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ =====
@app.route('/', methods=['GET'])
def root():
    """ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹"""
    logger.info("ğŸŒ ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚¢ã‚¯ã‚»ã‚¹")
    return jsonify({
        'status': 'healthy',
        'service': 'Complete Cloud Health Management Server',
        'version': '3.1',
        'debug_mode': 'csv_analysis_enhanced',
        'features': ['HAE Reception', 'Auto Analysis', 'LINE Notification', 'CSV Content Display'],
        'endpoints': {
            'health_data': '/health-data (POST)',
            'health_check': '/health-check (GET)',
            'latest_data': '/latest-data (GET)',
            'manual_analysis': '/manual-analysis (POST)',
            'csv_content': '/csv-content (GET)',
            'csv_dates': '/csv-dates (GET)'
        }
    })

@app.route('/health-data', methods=['POST'])
def receive_health_data():
    """HAEãƒ‡ãƒ¼ã‚¿å—ä¿¡ â†’ å³åº§çµ±åˆå‡¦ç†"""
    try:
        session_id = request.headers.get('session-id', 'unknown')
        data = request.get_json()
        
        logger.info(f"ğŸ¯ ===== HAEãƒ‡ãƒ¼ã‚¿å—ä¿¡ (Session: {session_id}) =====")
        
        if not data:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ãªã—")
            return jsonify({'error': 'No data received'}), 400
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"health_data_{timestamp}.json"
        filepath = os.path.join(DATA_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ HAEãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filename}")
        
        # ã€é‡è¦ã€‘å³åº§ã«çµ±åˆå‡¦ç†å®Ÿè¡Œ
        logger.info("ğŸš€ çµ±åˆå‡¦ç†é–‹å§‹")
        success = processor.process_hae_data_complete(data)
        
        metrics = data.get('data', {}).get('metrics', [])
        workouts = data.get('data', {}).get('workouts', [])
        
        logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {len(metrics)}å€‹")
        logger.info(f"ğŸƒ ãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¦ãƒˆ: {len(workouts)}å€‹")
        logger.info(f"ğŸ¯ å‡¦ç†çµæœ: {'âœ…æˆåŠŸ' if success else 'âŒå¤±æ•—'}")
        
        return jsonify({
            'status': 'success',
            'message': 'Data received and processed completely',
            'metrics_count': len(metrics),
            'workouts_count': len(workouts),
            'session_id': session_id,
            'processing_success': success,
            'features_executed': ['Data Conversion', 'CSV Integration', 'Health Analysis', 'LINE Notification'],
            'debug_info': {
                'timestamp': timestamp,
                'filename': filename,
                'data_dir': DATA_DIR,
                'reports_dir': REPORTS_DIR
            }
        })
    
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å—ä¿¡ãƒ»å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/health-check', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    logger.info("ğŸ” ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
    return jsonify({
        'status': 'OK',
        'version': '3.1',
        'timestamp': datetime.now().isoformat(),
        'line_configured': bool(LINE_BOT_TOKEN and LINE_USER_ID),
        'oura_configured': bool(OURA_TOKEN),
        'directories': {
            'data_dir': DATA_DIR,
            'reports_dir': REPORTS_DIR,
            'data_dir_exists': os.path.exists(DATA_DIR),
            'reports_dir_exists': os.path.exists(REPORTS_DIR)
        }
    })

@app.route('/latest-data', methods=['GET'])
def get_latest_data():
    """æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç¢ºèª"""
    try:
        logger.info("ğŸ“‹ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç¢ºèª")
        files = [f for f in os.listdir(DATA_DIR) if f.endswith('.json')]
        if not files:
            logger.warning("ğŸ“­ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            return jsonify({'message': 'No data files found'})
        
        latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(DATA_DIR, x)))
        
        with open(os.path.join(DATA_DIR, latest_file), 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"ğŸ“„ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return jsonify({
            'latest_file': latest_file,
            'data_preview': {
                'metrics_count': len(data.get('data', {}).get('metrics', [])),
                'workouts_count': len(data.get('data', {}).get('workouts', [])),
                'first_few_metrics': data.get('data', {}).get('metrics', [])[:3]
            }
        })
    
    except Exception as e:
        logger.error(f"âŒ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/manual-analysis', methods=['POST'])
def manual_analysis():
    """æ‰‹å‹•åˆ†æå®Ÿè¡Œ"""
    try:
        logger.info("ğŸ§  ===== æ‰‹å‹•åˆ†æå®Ÿè¡Œ =====")
        report = processor.analytics.analyze_health_data()
        
        if report:
            # LINEé€šçŸ¥é€ä¿¡
            notification_success = processor.notifier.send_health_report(report)
            
            logger.info(f"ğŸ“± LINEé€šçŸ¥: {'âœ…æˆåŠŸ' if notification_success else 'âŒå¤±æ•—'}")
            
            return jsonify({
                'status': 'success',
                'message': 'Manual analysis completed',
                'report': report,
                'line_notification': notification_success
            })
        else:
            logger.error("âŒ åˆ†æå¤±æ•—")
            return jsonify({'error': 'Analysis failed'}), 500
            
    except Exception as e:
        logger.error(f"âŒ æ‰‹å‹•åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': str(e)}), 500

# ===== CSVè¡¨ç¤ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰ =====
@app.route('/csv-content', methods=['GET'])
def get_csv_content():
    """CSVå†…å®¹è¡¨ç¤ºï¼ˆæ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãƒ»ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿ï¼‰"""
    try:
        logger.info("ğŸ“Š CSVå†…å®¹ç¢ºèªé–‹å§‹")
        
        reports_dir = Path(REPORTS_DIR)
        daily_csv = reports_dir / "æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿.csv"
        ma7_csv = reports_dir / "7æ—¥ç§»å‹•å¹³å‡ãƒ‡ãƒ¼ã‚¿.csv"
        index_csv = reports_dir / "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿.csv"
        
        result = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'files_status': {},
            'csv_data': {}
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        for name, path in [('daily', daily_csv), ('ma7', ma7_csv), ('index', index_csv)]:
            if path.exists():
                try:
                    df = pd.read_csv(path, encoding='utf-8-sig')
                    result['files_status'][name] = {
                        'exists': True,
                        'rows': len(df),
                        'columns': len(df.columns)
                    }
                    
                    # æœ€æ–°10è¡Œã‚’å–å¾—
                    if len(df) > 0:
                        latest_data = df.tail(10).to_dict('records')
                        result['csv_data'][name] = {
                            'columns': list(df.columns),
                            'latest_10_rows': latest_data,
                            'date_range': {
                                'first': df.iloc[0]['date'] if 'date' in df.columns else 'N/A',
                                'last': df.iloc[-1]['date'] if 'date' in df.columns else 'N/A'
                            }
                        }
                    else:
                        result['csv_data'][name] = {'message': 'Empty file'}
                        
                except Exception as e:
                    result['files_status'][name] = {
                        'exists': True,
                        'error': str(e)
                    }
            else:
                result['files_status'][name] = {'exists': False}
        
        logger.info(f"ğŸ“‹ CSVç¢ºèªå®Œäº†")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ CSVç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/csv-dates', methods=['GET'])
def get_csv_dates():
    """æŒ‡å®šæœŸé–“ã®CSVãƒ‡ãƒ¼ã‚¿ç¢ºèª"""
    try:
        start_date = request.args.get('start_date', '2025-08-08')
        end_date = request.args.get('end_date', '2025-08-11')
        
        logger.info(f"ğŸ“… æœŸé–“æŒ‡å®šCSVç¢ºèª: {start_date} - {end_date}")
        
        reports_dir = Path(REPORTS_DIR)
        daily_csv = reports_dir / "æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿.csv"
        
        if not daily_csv.exists():
            return jsonify({'error': 'Daily CSV file not found'}), 404
        
        df = pd.read_csv(daily_csv, encoding='utf-8-sig')
        
        # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            mask = (df['date'] >= start_date) & (df['date'] <= end_date)
            filtered_df = df.loc[mask]
            
            # æ—¥ä»˜ã‚’æ–‡å­—åˆ—ã«æˆ»ã™
            filtered_df = filtered_df.copy()
            filtered_df['date'] = filtered_df['date'].dt.strftime('%Y-%m-%d')
            
            result = {
                'period': f"{start_date} to {end_date}",
                'total_rows': len(filtered_df),
                'data': filtered_df.to_dict('records')
            }
            
            logger.info(f"ğŸ“Š æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(filtered_df)}è¡Œ")
            return jsonify(result)
        else:
            return jsonify({'error': 'Date column not found'}), 400
            
    except Exception as e:
        logger.error(f"âŒ æœŸé–“CSVç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({'error': str(e)}), 500

# ===== ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– =====
def initialize_app():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
    logger.info("ğŸŒ Complete Cloud Health Management Server v3.1 - CSV Enhanced")
    logger.info("=" * 60)
    logger.info("ğŸ¯ æ©Ÿèƒ½: HAEå—ä¿¡ â†’ å¤‰æ› â†’ çµ±åˆ â†’ åˆ†æ â†’ LINEé€šçŸ¥ï¼ˆå®Œå…¨è‡ªå‹•ï¼‰")
    logger.info("ğŸ“Š æ–°æ©Ÿèƒ½: CSVå†…å®¹è¡¨ç¤ºãƒ»æœŸé–“æŒ‡å®šãƒ‡ãƒ¼ã‚¿ç¢ºèª")
    logger.info(f"ğŸ“± LINEè¨­å®š: {'âœ…å®Œäº†' if LINE_BOT_TOKEN and LINE_USER_ID else 'âŒè¦è¨­å®š'}")
    logger.info(f"ğŸ” Ouraè¨­å®š: {'âœ…å®Œäº†' if OURA_TOKEN else 'âŒæœªè¨­å®š'}")
    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {DATA_DIR}")
    logger.info(f"ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {REPORTS_DIR}")
    logger.info("=" * 60)
    logger.info("âœ… çµ±åˆã‚µãƒ¼ãƒãƒ¼v3.1èµ·å‹•å®Œäº†")

# ===== ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚å‡¦ç† =====
initialize_app()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    
    logger.info(f"ğŸš€ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰èµ·å‹•: http://localhost:{port}")
    logger.info(f"ğŸ“¡ HAEé€ä¿¡å…ˆ: http://localhost:{port}/health-data")
    logger.info(f"ğŸ” ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: http://localhost:{port}/health-check")
    logger.info(f"ğŸ“Š æ‰‹å‹•åˆ†æ: http://localhost:{port}/manual-analysis (POST)")
    logger.info(f"ğŸ“‹ CSVå†…å®¹ç¢ºèª: http://localhost:{port}/csv-content")
    logger.info(f"ğŸ“… æœŸé–“ãƒ‡ãƒ¼ã‚¿ç¢ºèª: http://localhost:{port}/csv-dates?start_date=2025-08-08&end_date=2025-08-11")
    
    app.run(host='0.0.0.0', port=port, debug=False)